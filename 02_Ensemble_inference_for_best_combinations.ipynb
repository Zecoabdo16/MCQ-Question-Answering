{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.015828,"end_time":"2023-08-14T10:17:39.007683","exception":false,"start_time":"2023-08-14T10:17:38.991855","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:33:04.639281Z","iopub.execute_input":"2023-09-04T09:33:04.639632Z","iopub.status.idle":"2023-09-04T09:35:10.696419Z","shell.execute_reply.started":"2023-09-04T09:33:04.639604Z","shell.execute_reply":"2023-09-04T09:35:10.695231Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nProcessing ./sentence-transformers\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126134 sha256=be368231afe1403a4374877a9e392ebe39365facd8deea77306a4b250cb093e8\n  Stored in directory: /root/.cache/pip/wheels/6c/ea/76/d9a930b223b1d3d5d6aff69458725316b0fe205b854faf1812\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\nProcessing /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\nInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\nProcessing /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.2\n    Uninstalling transformers-4.30.2:\n      Successfully uninstalled transformers-4.30.2\nSuccessfully installed transformers-4.31.0\nProcessing /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nProcessing /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.14.3\nProcessing /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\nInstalling collected packages: trl\nSuccessfully installed trl-0.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\nfrom collections.abc import Iterable\nimport faiss\nfrom faiss import write_index, read_index\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:10.700750Z","iopub.execute_input":"2023-09-04T09:35:10.701070Z","iopub.status.idle":"2023-09-04T09:35:24.864291Z","shell.execute_reply.started":"2023-09-04T09:35:10.701043Z","shell.execute_reply":"2023-09-04T09:35:24.863299Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 9,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:24.869003Z","iopub.execute_input":"2023-09-04T09:35:24.871233Z","iopub.status.idle":"2023-09-04T09:35:24.881805Z","shell.execute_reply.started":"2023-09-04T09:35:24.871198Z","shell.execute_reply":"2023-09-04T09:35:24.880354Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:24.886212Z","iopub.execute_input":"2023-09-04T09:35:24.888723Z","iopub.status.idle":"2023-09-04T09:35:24.907058Z","shell.execute_reply.started":"2023-09-04T09:35:24.888690Z","shell.execute_reply":"2023-09-04T09:35:24.905989Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 9,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:24.909554Z","iopub.execute_input":"2023-09-04T09:35:24.913160Z","iopub.status.idle":"2023-09-04T09:35:24.929231Z","shell.execute_reply.started":"2023-09-04T09:35:24.913127Z","shell.execute_reply":"2023-09-04T09:35:24.928183Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:24.932833Z","iopub.execute_input":"2023-09-04T09:35:24.933610Z","iopub.status.idle":"2023-09-04T09:35:24.952919Z","shell.execute_reply.started":"2023-09-04T09:35:24.933579Z","shell.execute_reply":"2023-09-04T09:35:24.951916Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trn = pd.concat([\n    pd.read_csv('/kaggle/input/sim-data-2/extra_val.csv'),\n    pd.read_csv('/kaggle/input/sim-data-2/extra_eval_mos.csv'),\n    pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv'),\n])\n\ntrn = trn[[ 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']]","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:35:24.954366Z","iopub.execute_input":"2023-09-04T09:35:24.954999Z","iopub.status.idle":"2023-09-04T09:35:25.015430Z","shell.execute_reply.started":"2023-09-04T09:35:24.954967Z","shell.execute_reply":"2023-09-04T09:35:25.014503Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:36:16.423245Z","iopub.execute_input":"2023-09-04T09:36:16.423640Z","iopub.status.idle":"2023-09-04T09:36:19.164267Z","shell.execute_reply.started":"2023-09-04T09:36:16.423613Z","shell.execute_reply":"2023-09-04T09:36:19.163209Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:36:19.367418Z","iopub.execute_input":"2023-09-04T09:36:19.368456Z","iopub.status.idle":"2023-09-04T09:37:48.165442Z","shell.execute_reply.started":"2023-09-04T09:36:19.368416Z","shell.execute_reply":"2023-09-04T09:37:48.164324Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:37:48.167901Z","iopub.execute_input":"2023-09-04T09:37:48.168324Z","iopub.status.idle":"2023-09-04T09:37:55.953416Z","shell.execute_reply.started":"2023-09-04T09:37:48.168287Z","shell.execute_reply":"2023-09-04T09:37:55.952418Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f862ed10cc944fb2ba3ea5d396c697c2"}},"metadata":{}}]},{"cell_type":"code","source":"search_score, search_index = sentence_index.search(prompt_embeddings, 6)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:37:55.954919Z","iopub.execute_input":"2023-09-04T09:37:55.955278Z","iopub.status.idle":"2023-09-04T09:38:26.685204Z","shell.execute_reply.started":"2023-09-04T09:37:55.955247Z","shell.execute_reply":"2023-09-04T09:38:26.684337Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"del sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:38:26.690087Z","iopub.execute_input":"2023-09-04T09:38:26.691990Z","iopub.status.idle":"2023-09-04T09:38:27.593971Z","shell.execute_reply.started":"2023-09-04T09:38:26.691955Z","shell.execute_reply":"2023-09-04T09:38:27.592956Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:38:27.595278Z","iopub.execute_input":"2023-09-04T09:38:27.595621Z","iopub.status.idle":"2023-09-04T09:38:33.200308Z","shell.execute_reply.started":"2023-09-04T09:38:27.595588Z","shell.execute_reply":"2023-09-04T09:38:33.199204Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:38:33.202026Z","iopub.execute_input":"2023-09-04T09:38:33.202393Z","iopub.status.idle":"2023-09-04T09:38:34.105326Z","shell.execute_reply.started":"2023-09-04T09:38:33.202359Z","shell.execute_reply":"2023-09-04T09:38:34.104221Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/298 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b917a986b42436bb4a2798aeb3f3072"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"wiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:38:34.106747Z","iopub.execute_input":"2023-09-04T09:38:34.107206Z","iopub.status.idle":"2023-09-04T09:43:14.101203Z","shell.execute_reply.started":"2023-09-04T09:38:34.107173Z","shell.execute_reply":"2023-09-04T09:43:14.100141Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ae63838e734918b51e05f91302bbe9"}},"metadata":{}}]},{"cell_type":"code","source":"processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:43:14.102627Z","iopub.execute_input":"2023-09-04T09:43:14.103033Z","iopub.status.idle":"2023-09-04T09:43:25.648186Z","shell.execute_reply.started":"2023-09-04T09:43:14.102998Z","shell.execute_reply":"2023-09-04T09:43:25.647064Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1674 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d53e44a516496fb6112e9da50c0a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1674 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cceb15f1e5b43f698e949d7d3e3b882"}},"metadata":{}}]},{"cell_type":"code","source":"wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:43:25.650027Z","iopub.execute_input":"2023-09-04T09:43:25.650655Z","iopub.status.idle":"2023-09-04T09:44:10.401249Z","shell.execute_reply.started":"2023-09-04T09:43:25.650615Z","shell.execute_reply":"2023-09-04T09:44:10.400196Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2716 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fadaebe2eb4457ab52f111ce5bfa114"}},"metadata":{}}]},{"cell_type":"code","source":"_ = gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:44:10.404577Z","iopub.execute_input":"2023-09-04T09:44:10.404976Z","iopub.status.idle":"2023-09-04T09:44:10.726911Z","shell.execute_reply.started":"2023-09-04T09:44:10.404925Z","shell.execute_reply":"2023-09-04T09:44:10.725650Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:44:10.728611Z","iopub.execute_input":"2023-09-04T09:44:10.729027Z","iopub.status.idle":"2023-09-04T09:44:10.749363Z","shell.execute_reply.started":"2023-09-04T09:44:10.728992Z","shell.execute_reply":"2023-09-04T09:44:10.748409Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:47:48.988619Z","iopub.execute_input":"2023-09-04T09:47:48.989046Z","iopub.status.idle":"2023-09-04T09:47:49.398629Z","shell.execute_reply.started":"2023-09-04T09:47:48.989013Z","shell.execute_reply":"2023-09-04T09:47:49.397546Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b954a4a7ec44b8a9784a63349d907f"}},"metadata":{}}]},{"cell_type":"code","source":"## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 22\n\n## List containing just Context\ncontexts = []\n\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    if prompt_indices.shape[0] > 0:\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n        \n    contexts.append(context)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:47:50.565960Z","iopub.execute_input":"2023-09-04T09:47:50.566356Z","iopub.status.idle":"2023-09-04T09:47:58.554555Z","shell.execute_reply.started":"2023-09-04T09:47:50.566319Z","shell.execute_reply":"2023-09-04T09:47:58.553495Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/298 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57d6a0127f3a4e879f59439da4ec603f"}},"metadata":{}}]},{"cell_type":"code","source":"trn['context'] = contexts\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:47:58.556923Z","iopub.execute_input":"2023-09-04T09:47:58.557370Z","iopub.status.idle":"2023-09-04T09:47:58.564209Z","shell.execute_reply.started":"2023-09-04T09:47:58.557330Z","shell.execute_reply":"2023-09-04T09:47:58.562826Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\",'answer']].to_csv(\"./test_context.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:47:58.565781Z","iopub.execute_input":"2023-09-04T09:47:58.566414Z","iopub.status.idle":"2023-09-04T09:47:58.652647Z","shell.execute_reply.started":"2023-09-04T09:47:58.566376Z","shell.execute_reply":"2023-09-04T09:47:58.651654Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_context.csv\")\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"] + \" #### \" +  test_df[\"prompt\"]\nvalid_label = test_df.loc[:, 'answer'].values","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:48:22.682895Z","iopub.execute_input":"2023-09-04T09:48:22.683307Z","iopub.status.idle":"2023-09-04T09:48:22.713743Z","shell.execute_reply.started":"2023-09-04T09:48:22.683266Z","shell.execute_reply":"2023-09-04T09:48:22.712778Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# test_df = pd.read_csv(\"/kaggle/input/sim-data-2/val298_context.csv\")\n# test_df.index = list(range(len(test_df)))\n# test_df['id'] = list(range(len(test_df)))\n# # test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"]\n# valid_label = test_df.loc[:, 'answer'].values","metadata":{"papermill":{"duration":0.037633,"end_time":"2023-08-14T10:17:39.605345","exception":false,"start_time":"2023-08-14T10:17:39.567712","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-02T07:46:17.003520Z","iopub.execute_input":"2023-09-02T07:46:17.003961Z","iopub.status.idle":"2023-09-02T07:46:17.039265Z","shell.execute_reply.started":"2023-09-02T07:46:17.003927Z","shell.execute_reply":"2023-09-02T07:46:17.038380Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_df['prompt'][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:48:28.796170Z","iopub.execute_input":"2023-09-04T09:48:28.796558Z","iopub.status.idle":"2023-09-04T09:48:28.805165Z","shell.execute_reply.started":"2023-09-04T09:48:28.796530Z","shell.execute_reply":"2023-09-04T09:48:28.804098Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'Methanol acquired the name wood alcohol because it was once produced chiefly by the destructive distillation of wood. Methanol (also called methyl alcohol and wood spirit, amongst other names) is an organic chemical and the simplest aliphatic alcohol, with the formula CH3OH (a methyl group linked to a hydroxyl group, often abbreviated as MeOH). Grade \"AA\" methanol contains trace amounts of ethanol as well. This addition of methanol exempts industrial ethanol (commonly known as \"denatured alcohol\" or \"methylated spirit\") from liquor excise taxation in the U.S. and other countries. ==See also== *Aminomethanol *Methanol (data page) *Trimethyl carbinol ==References== ==Further reading== *Robert Boyle, The Sceptical Chymist (1661) – contains account of distillation of wood alcohol. ==External links== * * Methyl Alcohol (Methanol) CDC/NIOSH, links to safety information * CDC – NIOSH Pocket Guide to Chemical Hazards – Methyl Alcohol * Methanol Fact Sheet – National Pollutant Inventory Category:Alkanols Category:Alcohol solvents Category:Anatomical preservation Category:Biofuels Category:Energy storage Category:Hazardous air pollutants Category:Human metabolites Category:Neurotoxins Category:Oxygenates Category:Commodity chemicals Category:GABAA receptor positive allosteric modulators They then called wood alcohol (l\\'esprit de bois) \"bihydrate de méthylène\" (bihydrate because they thought the formula was C4H8O4 = (CH)4(H2O)2). Pinacolyl alcohol is a common name for 3,3-dimethylbutan-2-ol, also known as pine alcohol. Because of its similarities in both appearance and odor to ethanol (the alcohol in beverages), it is difficult to differentiate between the two; such is also the case with denatured alcohol, adulterated liquors or very low quality alcoholic beverages. Amyl alcohols are alcohols with the formula C5H11OH.Merriam-Webster\\'s Collegiate Dictionary 11th Ed. 2004 Eight are known. For this reason wood can be considered to be an essentially carbon-neutral source of energy. == Drawbacks == While it seems reasonable that Treethanol could be an alternative to current ethanol types, it has one flaw, which is the extra processing needed to break down the tough cellulose and hemicellulose within the walls of the cell to isolate the sugars. It is one of the isomeric hexanols and a secondary alcohol. Because of its toxic properties, methanol is frequently used as a denaturant additive for ethanol manufactured for industrial uses. Linoleyl alcohol is a fatty alcohol. (1) methy, wine, and hulē, wood; that is, wine or spirit of wood.) Methanol is mixed with water and injected into high performance diesel and gasoline engines for an increase of power and a decrease in intake air temperature in a process known as water methanol injection. ====Other applications==== Methanol is used as a denaturant for ethanol, the product being known as \"denatured alcohol\" or \"methylated spirit\". Methanol for chemical use normally corresponds to Grade AA. Methanol is also a widely used fuel in camping and boating stoves. The most important amyl alcohol is isoamyl alcohol, the chief one generated by fermentation in the production of alcoholic beverages and a constituent of fusel oil. Treethanol is an ethanol fuel (more precisely cellulosic ethanol) made from trees. == Summary == The biofuel is a contender in the race to find an energy alternative to fossil fuels. Sugar alcohols (also called polyhydric alcohols, polyalcohols, alditols or glycitols) are organic compounds, typically derived from sugars, containing one hydroxyl group (–OH) attached to each carbon atom. The name amyl alcohol without further specification applies to the normal (straight-chain) form, 1-pentanol. Pinacolyl alcohol appears on the List of Schedule 2 substances (CWC) as a precursor for the nerve agent soman. ==See also== *Soman *Isopropyl alcohol ==References== ==External links== * IPCS INTOX Databank Category:Hexanols Category:Nerve agent precursors Category:Secondary alcohols A mixture of amyl alcohols (also called amyl alcohol) can be obtained from fusel alcohol.  #### Which of the following is also known as wood alcohol?'"},"metadata":{}}]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example","metadata":{"papermill":{"duration":0.026162,"end_time":"2023-08-14T10:18:01.129276","exception":false,"start_time":"2023-08-14T10:18:01.103114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-04T09:48:37.980194Z","iopub.execute_input":"2023-09-04T09:48:37.980835Z","iopub.status.idle":"2023-09-04T09:48:37.989837Z","shell.execute_reply.started":"2023-09-04T09:48:37.980800Z","shell.execute_reply":"2023-09-04T09:48:37.988601Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"papermill":{"duration":0.030447,"end_time":"2023-08-14T10:18:01.175589","exception":false,"start_time":"2023-08-14T10:18:01.145142","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-04T09:48:39.348874Z","iopub.execute_input":"2023-09-04T09:48:39.350120Z","iopub.status.idle":"2023-09-04T09:48:39.360673Z","shell.execute_reply.started":"2023-09-04T09:48:39.350078Z","shell.execute_reply":"2023-09-04T09:48:39.359672Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:48:40.771005Z","iopub.execute_input":"2023-09-04T09:48:40.771599Z","iopub.status.idle":"2023-09-04T09:48:40.778427Z","shell.execute_reply.started":"2023-09-04T09:48:40.771565Z","shell.execute_reply":"2023-09-04T09:48:40.775348Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T09:48:41.285047Z","iopub.execute_input":"2023-09-04T09:48:41.286083Z","iopub.status.idle":"2023-09-04T09:48:41.648809Z","shell.execute_reply.started":"2023-09-04T09:48:41.286050Z","shell.execute_reply":"2023-09-04T09:48:41.647787Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"papermill":{"duration":0.493618,"end_time":"2023-08-14T10:18:01.685989","exception":false,"start_time":"2023-08-14T10:18:01.192371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-04T09:48:42.205929Z","iopub.execute_input":"2023-09-04T09:48:42.206635Z","iopub.status.idle":"2023-09-04T09:48:47.569876Z","shell.execute_reply.started":"2023-09-04T09:48:42.206601Z","shell.execute_reply":"2023-09-04T09:48:47.568973Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19187b5c1a9548a98f22b3f54a8857cb"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_807 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_807.append(outputs.logits.cpu().detach())\n\ntest_807 = torch.cat(test_807)\n\n\n\nmodel_dir = \"/kaggle/input/llm-se-debertav3-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_771 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_771.append(outputs.logits.cpu().detach())\n\ntest_771 = torch.cat(test_771)\n\n\nmodel_dir = \"/kaggle/input/zak-scratchfrozen-context200/Zak_Frozen_context\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_788 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_788.append(outputs.logits.cpu().detach())\n\ntest_788 = torch.cat(test_788)\n\n\nmodel_dir = \"/kaggle/input/the-cute-context200/Zak_Frozen_context_Cute\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_791 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_791.append(outputs.logits.cpu().detach())\n\ntest_791 = torch.cat(test_791)","metadata":{"papermill":{"duration":21.360878,"end_time":"2023-08-14T10:18:01.027859","exception":false,"start_time":"2023-08-14T10:17:39.666981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-04T09:48:47.574226Z","iopub.execute_input":"2023-09-04T09:48:47.576677Z","iopub.status.idle":"2023-09-04T10:12:00.074670Z","shell.execute_reply.started":"2023-09-04T09:48:47.576640Z","shell.execute_reply":"2023-09-04T10:12:00.073594Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmodel_dir = \"/kaggle/input/frozen160clip/Zak_Frozen_context\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_p784 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_p784.append(outputs.logits.cpu().detach())\n\ntest_p784 = torch.cat(test_p784)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T10:12:00.076495Z","iopub.execute_input":"2023-09-04T10:12:00.076859Z","iopub.status.idle":"2023-09-04T10:17:47.213083Z","shell.execute_reply.started":"2023-09-04T10:12:00.076826Z","shell.execute_reply":"2023-09-04T10:17:47.211991Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/frozen160clip/Zak_Frozen_context\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\ntest_p779 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_p779.append(outputs.logits.cpu().detach())\n\ntest_p779 = torch.cat(test_p779)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T10:17:47.215919Z","iopub.execute_input":"2023-09-04T10:17:47.216298Z","iopub.status.idle":"2023-09-04T10:23:15.693309Z","shell.execute_reply.started":"2023-09-04T10:17:47.216264Z","shell.execute_reply":"2023-09-04T10:23:15.692201Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/new-partner/model_v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\nnew_partner = []\n\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    new_partner.append(outputs.logits.cpu().detach())\n\nnew_partner = torch.cat(new_partner)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T10:23:15.695027Z","iopub.execute_input":"2023-09-04T10:23:15.695765Z","iopub.status.idle":"2023-09-04T10:29:01.889670Z","shell.execute_reply.started":"2023-09-04T10:23:15.695719Z","shell.execute_reply":"2023-09-04T10:29:01.888549Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"test_df = pd.concat([\n    pd.read_csv('/kaggle/input/sim-data-2/extra_val.csv'),\n    pd.read_csv('/kaggle/input/sim-data-2/extra_eval_mos.csv'),\n    pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv'),\n])\ntokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T08:41:27.564074Z","iopub.execute_input":"2023-09-02T08:41:27.564434Z","iopub.status.idle":"2023-09-02T08:41:28.067780Z","shell.execute_reply.started":"2023-09-02T08:41:27.564406Z","shell.execute_reply":"2023-09-02T08:41:28.066770Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/298 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb8f9d81e204efe987a53bdf83ec781"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/2023kagglellm-deberta-v3-large-model1').cuda()\nmodel.eval()\npreds = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    preds.append(outputs.logits.cpu().detach())\n\ntest756 = torch.cat(preds)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T08:41:32.926513Z","iopub.execute_input":"2023-09-02T08:41:32.926910Z","iopub.status.idle":"2023-09-02T08:42:09.795002Z","shell.execute_reply.started":"2023-09-02T08:41:32.926881Z","shell.execute_reply":"2023-09-02T08:42:09.793994Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"}]},{"cell_type":"code","source":"def map3(y_true, y_pred):\n    m = (y_true.reshape((-1,1)) == y_pred)\n    return np.mean(np.where(m.any(axis=1), m.argmax(axis=1)+1, np.inf)**(-1))","metadata":{"execution":{"iopub.status.busy":"2023-09-04T10:29:01.891449Z","iopub.execute_input":"2023-09-04T10:29:01.891822Z","iopub.status.idle":"2023-09-04T10:29:01.898233Z","shell.execute_reply.started":"2023-09-04T10:29:01.891780Z","shell.execute_reply":"2023-09-04T10:29:01.897038Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"valid_pred_ids = np.argsort(-test_p779, 1)\nvalid_pred_letters = np.array(list('ABCDE'))[valid_pred_ids][:, :3]\nvalid_map3 = map3(valid_label, valid_pred_letters)\nprint(valid_map3)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T10:31:51.163040Z","iopub.execute_input":"2023-09-04T10:31:51.163449Z","iopub.status.idle":"2023-09-04T10:31:51.171128Z","shell.execute_reply.started":"2023-09-04T10:31:51.163417Z","shell.execute_reply":"2023-09-04T10:31:51.170148Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"0.7555928411633109\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_807    0.860738255033557\n# test_791    0.8288590604026845\n# test_p784   0.8182326621923937\n# test_788    0.8126398210290828\n# test_p779   0.8182326621923937\n# test_771    0.7908277404921701\n# test756     0.8036912751677852\n# new_partner 0.7841163310961969","metadata":{"execution":{"iopub.status.busy":"2023-08-29T07:40:11.239762Z","iopub.status.idle":"2023-08-29T07:40:11.240167Z","shell.execute_reply.started":"2023-08-29T07:40:11.239950Z","shell.execute_reply":"2023-08-29T07:40:11.239967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Loop time ! \n","metadata":{}},{"cell_type":"markdown","source":"4 models","metadata":{}},{"cell_type":"code","source":"for s in range (0,100):\n  a = 100 - s\n  for i in range (a):\n    z = a - i\n    \n#     predictions = ( s * test_807 + i * test_791  +  z * new_partner ) / 100     \n#     0.8780760626398211 22 45 33\n    predictions = ( s * test_807 + i * test_791  +  z * new_partner ) / 100     \n\n    valid_pred_ids = np.argsort(-predictions, 1)\n    valid_pred_letters = np.array(list('ABCDE'))[valid_pred_ids][:, :3]\n    valid_map3 = map3(valid_label, valid_pred_letters)\n\n    if valid_map3 > 0.868:\n        print(valid_map3 ,s, i , z)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T08:16:12.282782Z","iopub.execute_input":"2023-09-02T08:16:12.283152Z","iopub.status.idle":"2023-09-02T08:16:13.847717Z","shell.execute_reply.started":"2023-09-02T08:16:12.283123Z","shell.execute_reply":"2023-09-02T08:16:13.846643Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.8680089485458614 31 24 45\n0.8685682326621925 31 25 44\n0.8680089485458614 31 32 37\n0.8680089485458614 31 33 36\n0.8680089485458612 32 20 48\n0.8680089485458612 32 21 47\n0.8680089485458612 32 22 46\n0.8680089485458614 32 25 43\n0.8680089485458614 36 26 38\n0.8680089485458612 55 27 18\n0.8680089485458612 56 25 19\n0.8680089485458612 56 26 18\n0.8680089485458612 56 27 17\n0.8680089485458612 57 25 18\n0.8680089485458612 57 26 17\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_807    0.860738255033557\n# test_791    0.8288590604026845\n# test_p784   0.8182326621923937\n# test_788    0.8126398210290828\n# test_p779   0.8182326621923937\n# test_771    0.7908277404921701\n# test756     0.8036912751677852\n# new_partner 0.7841163310961969","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for s in range (100):\n  a = 100 - s\n  for i in range (a):\n    z = a - i\n    for m in range (z):\n      x = z - m\n      # print (s+ i + m + x)\n# 0.8691275167785235 38 19 40 3\n# 0.8825503355704698 15 28 22 35\n\n      # test_pred = ((i * test_pred_161_dash ) + (m * test_smsm_eff) + (x * test_smsm)) / 100\n      predictions = ( x * test_807 + i * test756  +  m * new_partner +  s * test_791) / 100     \n\n      valid_pred_ids = np.argsort(-predictions, 1)\n      valid_pred_letters = np.array(list('ABCDE'))[valid_pred_ids][:, :3]\n      valid_map3 = map3(valid_label, valid_pred_letters)\n        \n      if valid_map3 >=  0.881431767337807  :\n        print(valid_map3 ,x, i , m , s)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T08:47:04.845665Z","iopub.execute_input":"2023-09-02T08:47:04.846027Z","iopub.status.idle":"2023-09-02T08:47:57.649485Z","shell.execute_reply.started":"2023-09-02T08:47:04.846000Z","shell.execute_reply":"2023-09-02T08:47:57.648515Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"0.8814317673378077 14 35 24 27\n0.8825503355704698 14 31 23 32\n0.8814317673378077 12 31 25 32\n0.8814317673378077 13 29 25 33\n0.8814317673378077 14 31 22 33\n0.8814317673378077 13 29 24 34\n0.8819910514541388 11 29 26 34\n0.8814317673378077 14 32 20 34\n0.8814317673378077 13 33 20 34\n0.8825503355704698 15 28 22 35\n0.8814317673378077 13 29 23 35\n0.8814317673378077 13 32 20 35\n0.8814317673378077 13 33 19 35\n0.8814317673378077 12 34 19 35\n0.8814317673378077 8 48 1 43\n","output_type":"stream"}]},{"cell_type":"markdown","source":"5 models","metadata":{}},{"cell_type":"code","source":"for s in range(0, 100):\n  a = 100 - s\n  for i in range(a):\n    z = a - i\n    for m in range(z):\n      x = z - m\n      for d in range(x):\n        b = x - d  \n        print (s+ i + m + d + b)\n\n      # test_pred = ((i * test_pred_161_dash ) + (m * test_smsm_eff) + (x * test_smsm)) / 100\n        predictions = ( s * test_807 + i * test_791  +  m * new_partner +  d * test756 + b * test_p784) / 100  \n\n\n        valid_pred_ids = np.argsort(-predictions, 1)\n        valid_pred_letters = np.array(list('ABCDE'))[valid_pred_ids][:, :3]\n        valid_map3 = map3(valid_label, valid_pred_letters)\n\n        if valid_map3 >= 0.882:\n            print(valid_map3 ,s, i , m , d , b)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T08:50:57.117348Z","iopub.execute_input":"2023-09-02T08:50:57.117883Z","iopub.status.idle":"2023-09-02T09:14:41.490847Z","shell.execute_reply.started":"2023-09-02T08:50:57.117844Z","shell.execute_reply":"2023-09-02T09:14:41.489780Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"0.8825503355704698 14 31 23 31 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test_predictions = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)\n\npredictions_as_ids = np.argsort(-test_predictions, 1)\n\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n# predictions_as_answer_letters[:3]\n\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{"papermill":{"duration":1.101895,"end_time":"2023-08-14T10:18:02.804298","exception":false,"start_time":"2023-08-14T10:18:01.702403","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-29T07:40:11.246721Z","iopub.status.idle":"2023-08-29T07:40:11.247327Z","shell.execute_reply.started":"2023-08-29T07:40:11.246997Z","shell.execute_reply":"2023-08-29T07:40:11.247043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.033576,"end_time":"2023-08-14T10:19:17.733491","exception":false,"start_time":"2023-08-14T10:19:17.699915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-29T07:40:11.249004Z","iopub.status.idle":"2023-08-29T07:40:11.249586Z","shell.execute_reply.started":"2023-08-29T07:40:11.249292Z","shell.execute_reply":"2023-08-29T07:40:11.249318Z"},"trusted":true},"execution_count":null,"outputs":[]}]}